{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HTTP_PROXY']=\"http://www-cache.rd.bbc.co.uk:8080\"\n",
    "os.environ['HTTPS_PROXY']=\"https://www-cache.rd.bbc.co.uk:8080\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# %load network.py\n",
    "\"\"\"Class that represents the network to be evolved.\"\"\"\n",
    "import random\n",
    "import logging\n",
    "from train import train_and_score\n",
    "\n",
    "class Network():\n",
    "    \"\"\"Represent a network and let us operate on it.\n",
    "\n",
    "    Currently only works for an MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nn_param_choices=None):\n",
    "        \"\"\"Initialize our network.\n",
    "\n",
    "        Args:\n",
    "            nn_param_choices (dict): Parameters for the network, includes:\n",
    "                nb_neurons (list): [64, 128, 256]\n",
    "                nb_layers (list): [1, 2, 3, 4]\n",
    "                activation (list): ['relu', 'elu']\n",
    "                optimizer (list): ['rmsprop', 'adam']\n",
    "        \"\"\"\n",
    "        self.accuracy = 0.\n",
    "        self.nn_param_choices = nn_param_choices\n",
    "        self.network = {}  # (dic): represents MLP network parameters\n",
    "\n",
    "    def create_random(self):\n",
    "        \"\"\"Create a random network.\"\"\"\n",
    "        for key in self.nn_param_choices:\n",
    "            self.network[key] = random.choice(self.nn_param_choices[key])\n",
    "\n",
    "    def create_set(self, network):\n",
    "        \"\"\"Set network properties.\n",
    "\n",
    "        Args:\n",
    "            network (dict): The network parameters\n",
    "\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "\n",
    "    def train(self, dataset):\n",
    "        \"\"\"Train the network and record the accuracy.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of dataset to use.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.accuracy == 0.:\n",
    "            self.accuracy = train_and_score(self.network, dataset)\n",
    "\n",
    "    def print_network(self):\n",
    "        \"\"\"Print out a network.\"\"\"\n",
    "        logging.info(self.network)\n",
    "        logging.info(\"Network accuracy: %.2f%%\" % (self.accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load optimizer.py\n",
    "\"\"\"\n",
    "Class that holds a genetic algorithm for evolving a network.\n",
    "\n",
    "Credit:\n",
    "    A lot of those code was originally inspired by:\n",
    "    http://lethain.com/genetic-algorithms-cool-name-damn-simple/\n",
    "\"\"\"\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "import random\n",
    "from network import Network\n",
    "\n",
    "class Optimizer():\n",
    "    \"\"\"Class that implements genetic algorithm for MLP optimization.\"\"\"\n",
    "\n",
    "    def __init__(self, nn_param_choices, retain=0.4,\n",
    "                 random_select=0.1, mutate_chance=0.2):\n",
    "        \"\"\"Create an optimizer.\n",
    "\n",
    "        Args:\n",
    "            nn_param_choices (dict): Possible network paremters\n",
    "            retain (float): Percentage of population to retain after\n",
    "                each generation\n",
    "            random_select (float): Probability of a rejected network\n",
    "                remaining in the population\n",
    "            mutate_chance (float): Probability a network will be\n",
    "                randomly mutated\n",
    "\n",
    "        \"\"\"\n",
    "        self.mutate_chance = mutate_chance\n",
    "        self.random_select = random_select\n",
    "        self.retain = retain\n",
    "        self.nn_param_choices = nn_param_choices\n",
    "\n",
    "    def create_population(self, count):\n",
    "        \"\"\"Create a population of random networks.\n",
    "\n",
    "        Args:\n",
    "            count (int): Number of networks to generate, aka the\n",
    "                size of the population\n",
    "\n",
    "        Returns:\n",
    "            (list): Population of network objects\n",
    "\n",
    "        \"\"\"\n",
    "        pop = []\n",
    "        for _ in range(0, count):\n",
    "            # Create a random network.\n",
    "            network = Network(self.nn_param_choices)\n",
    "            network.create_random()\n",
    "\n",
    "            # Add the network to our population.\n",
    "            pop.append(network)\n",
    "\n",
    "        return pop\n",
    "\n",
    "    @staticmethod\n",
    "    def fitness(network):\n",
    "        \"\"\"Return the accuracy, which is our fitness function.\"\"\"\n",
    "        return network.accuracy\n",
    "\n",
    "    def grade(self, pop):\n",
    "        \"\"\"Find average fitness for a population.\n",
    "\n",
    "        Args:\n",
    "            pop (list): The population of networks\n",
    "\n",
    "        Returns:\n",
    "            (float): The average accuracy of the population\n",
    "\n",
    "        \"\"\"\n",
    "        summed = reduce(add, (self.fitness(network) for network in pop))\n",
    "        return summed / float((len(pop)))\n",
    "\n",
    "    def breed(self, mother, father):\n",
    "        \"\"\"Make two children as parts of their parents.\n",
    "\n",
    "        Args:\n",
    "            mother (dict): Network parameters\n",
    "            father (dict): Network parameters\n",
    "\n",
    "        Returns:\n",
    "            (list): Two network objects\n",
    "\n",
    "        \"\"\"\n",
    "        children = []\n",
    "        for _ in range(2):\n",
    "\n",
    "            child = {}\n",
    "\n",
    "            # Loop through the parameters and pick params for the kid.\n",
    "            for param in self.nn_param_choices:\n",
    "                child[param] = random.choice(\n",
    "                    [mother.network[param], father.network[param]]\n",
    "                )\n",
    "\n",
    "            # Now create a network object.\n",
    "            network = Network(self.nn_param_choices)\n",
    "            network.create_set(child)\n",
    "\n",
    "            children.append(network)\n",
    "\n",
    "        return children\n",
    "\n",
    "    def mutate(self, network):\n",
    "        \"\"\"Randomly mutate one part of the network.\n",
    "\n",
    "        Args:\n",
    "            network (dict): The network parameters to mutate\n",
    "\n",
    "        Returns:\n",
    "            (Network): A randomly mutated network object\n",
    "\n",
    "        \"\"\"\n",
    "        # Choose a random key.\n",
    "        mutation = random.choice(list(self.nn_param_choices.keys()))\n",
    "\n",
    "        # Mutate one of the params.\n",
    "        network.network[mutation] = random.choice(self.nn_param_choices[mutation])\n",
    "\n",
    "        return network\n",
    "\n",
    "    def evolve(self, pop):\n",
    "        \"\"\"Evolve a population of networks.\n",
    "\n",
    "        Args:\n",
    "            pop (list): A list of network parameters\n",
    "\n",
    "        Returns:\n",
    "            (list): The evolved population of networks\n",
    "\n",
    "        \"\"\"\n",
    "        # Get scores for each network.\n",
    "        graded = [(self.fitness(network), network) for network in pop]\n",
    "\n",
    "        # Sort on the scores.\n",
    "        graded = [x[1] for x in sorted(graded, key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "        # Get the number we want to keep for the next gen.\n",
    "        retain_length = int(len(graded)*self.retain)\n",
    "\n",
    "        # The parents are every network we want to keep.\n",
    "        parents = graded[:retain_length]\n",
    "\n",
    "        # For those we aren't keeping, randomly keep some anyway.\n",
    "        for individual in graded[retain_length:]:\n",
    "            if self.random_select > random.random():\n",
    "                parents.append(individual)\n",
    "\n",
    "        # Randomly mutate some of the networks we're keeping.\n",
    "        for individual in parents:\n",
    "            if self.mutate_chance > random.random():\n",
    "                individual = self.mutate(individual)\n",
    "\n",
    "        # Now find out how many spots we have left to fill.\n",
    "        parents_length = len(parents)\n",
    "        desired_length = len(pop) - parents_length\n",
    "        children = []\n",
    "\n",
    "        # Add children, which are bred from two remaining networks.\n",
    "        while len(children) < desired_length:\n",
    "\n",
    "            # Get a random mom and dad.\n",
    "            male = random.randint(0, parents_length-1)\n",
    "            female = random.randint(0, parents_length-1)\n",
    "\n",
    "            # Assuming they aren't the same network...\n",
    "            if male != female:\n",
    "                male = parents[male]\n",
    "                female = parents[female]\n",
    "\n",
    "                # Breed them.\n",
    "                babies = self.breed(male, female)\n",
    "\n",
    "                # Add the children one at a time.\n",
    "                for baby in babies:\n",
    "                    # Don't grow larger than desired length.\n",
    "                    if len(children) < desired_length:\n",
    "                        children.append(baby)\n",
    "\n",
    "        parents.extend(children)\n",
    "\n",
    "        return parents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load train.py\n",
    "\"\"\"\n",
    "Utility used by the Network class to actually train.\n",
    "\n",
    "Based on:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py\n",
    "\n",
    "\"\"\"\n",
    "from keras.datasets import mnist, cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Helper: Early stopping.\n",
    "early_stopper = EarlyStopping(patience=5)\n",
    "\n",
    "def get_cifar10():\n",
    "    \"\"\"Retrieve the CIFAR dataset and process the data.\"\"\"\n",
    "    # Set defaults.\n",
    "    nb_classes = 10\n",
    "    batch_size = 64\n",
    "    input_shape = (3072,)\n",
    "\n",
    "    # Get the data.\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train = x_train.reshape(50000, 3072)\n",
    "    x_test = x_test.reshape(10000, 3072)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = to_categorical(y_train, nb_classes)\n",
    "    y_test = to_categorical(y_test, nb_classes)\n",
    "\n",
    "    return (nb_classes, batch_size, input_shape, x_train, x_test, y_train, y_test)\n",
    "\n",
    "def get_mnist():\n",
    "    \"\"\"Retrieve the MNIST dataset and process the data.\"\"\"\n",
    "    # Set defaults.\n",
    "    nb_classes = 10\n",
    "    batch_size = 128\n",
    "    input_shape = (784,)\n",
    "\n",
    "    # Get the data.\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(60000, 784)\n",
    "    x_test = x_test.reshape(10000, 784)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = to_categorical(y_train, nb_classes)\n",
    "    y_test = to_categorical(y_test, nb_classes)\n",
    "\n",
    "    return (nb_classes, batch_size, input_shape, x_train, x_test, y_train, y_test)\n",
    "\n",
    "def compile_model(network, nb_classes, input_shape):\n",
    "    \"\"\"Compile a sequential model.\n",
    "\n",
    "    Args:\n",
    "        network (dict): the parameters of the network\n",
    "\n",
    "    Returns:\n",
    "        a compiled network.\n",
    "\n",
    "    \"\"\"\n",
    "    # Get our network parameters.\n",
    "    nb_layers = network['nb_layers']\n",
    "    nb_neurons = network['nb_neurons']\n",
    "    activation = network['activation']\n",
    "    optimizer = network['optimizer']\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add each layer.\n",
    "    for i in range(nb_layers):\n",
    "\n",
    "        # Need input shape for first layer.\n",
    "        if i == 0:\n",
    "            model.add(Dense(nb_neurons, activation=activation, input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(Dense(nb_neurons, activation=activation))\n",
    "\n",
    "        model.add(Dropout(0.2))  # hard-coded dropout\n",
    "\n",
    "    # Output layer.\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_and_score(network, dataset):\n",
    "    \"\"\"Train the model, return test loss.\n",
    "\n",
    "    Args:\n",
    "        network (dict): the parameters of the network\n",
    "        dataset (str): Dataset to use for training/evaluating\n",
    "\n",
    "    \"\"\"\n",
    "    if dataset == 'cifar10':\n",
    "        nb_classes, batch_size, input_shape, x_train, \\\n",
    "            x_test, y_train, y_test = get_cifar10()\n",
    "    elif dataset == 'mnist':\n",
    "        nb_classes, batch_size, input_shape, x_train, \\\n",
    "            x_test, y_train, y_test = get_mnist()\n",
    "\n",
    "    model = compile_model(network, nb_classes, input_shape)\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=10000,  # using early stopping, so no real limit\n",
    "              verbose=0,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks=[early_stopper])\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    return score[1]  # 1 is accuracy. 0 is loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 4/5 [19:56<05:19, 319.90s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-635268975604>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-635268975604>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m                  (generations, population))\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m     \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_param_choices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-635268975604>\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(generations, population, nn_param_choices, dataset)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m# Train and get accuracy for networks.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mtrain_networks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetworks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m# Get the average accuracy for this generation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-635268975604>\u001b[0m in \u001b[0;36mtrain_networks\u001b[1;34m(networks, dataset)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetworks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\machinelearning\\neural-network-genetic-algorithm\\network.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \"\"\"\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprint_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\machinelearning\\neural-network-genetic-algorithm\\train.py\u001b[0m in \u001b[0;36mtrain_and_score\u001b[1;34m(network, dataset)\u001b[0m\n\u001b[0;32m    118\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m               callbacks=[early_stopper])\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1507\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %load main.py\n",
    "\"\"\"Entry point to evolving the neural network. Start here.\"\"\"\n",
    "import logging\n",
    "from optimizer import Optimizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup logging.\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "    level=logging.DEBUG,\n",
    "    filename='log.txt'\n",
    ")\n",
    "\n",
    "def train_networks(networks, dataset):\n",
    "    \"\"\"Train each network.\n",
    "\n",
    "    Args:\n",
    "        networks (list): Current population of networks\n",
    "        dataset (str): Dataset to use for training/evaluating\n",
    "    \"\"\"\n",
    "    pbar = tqdm(total=len(networks))\n",
    "    for network in networks:\n",
    "        network.train(dataset)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "def get_average_accuracy(networks):\n",
    "    \"\"\"Get the average accuracy for a group of networks.\n",
    "\n",
    "    Args:\n",
    "        networks (list): List of networks\n",
    "\n",
    "    Returns:\n",
    "        float: The average accuracy of a population of networks.\n",
    "\n",
    "    \"\"\"\n",
    "    total_accuracy = 0\n",
    "    for network in networks:\n",
    "        total_accuracy += network.accuracy\n",
    "\n",
    "    return total_accuracy / len(networks)\n",
    "\n",
    "def generate(generations, population, nn_param_choices, dataset):\n",
    "    \"\"\"Generate a network with the genetic algorithm.\n",
    "\n",
    "    Args:\n",
    "        generations (int): Number of times to evole the population\n",
    "        population (int): Number of networks in each generation\n",
    "        nn_param_choices (dict): Parameter choices for networks\n",
    "        dataset (str): Dataset to use for training/evaluating\n",
    "\n",
    "    \"\"\"\n",
    "    optimizer = Optimizer(nn_param_choices)\n",
    "    networks = optimizer.create_population(population)\n",
    "\n",
    "    # Evolve the generation.\n",
    "    for i in range(generations):\n",
    "        logging.info(\"***Doing generation %d of %d***\" %\n",
    "                     (i + 1, generations))\n",
    "\n",
    "        # Train and get accuracy for networks.\n",
    "        train_networks(networks, dataset)\n",
    "\n",
    "        # Get the average accuracy for this generation.\n",
    "        average_accuracy = get_average_accuracy(networks)\n",
    "\n",
    "        # Print out the average accuracy each generation.\n",
    "        logging.info(\"Generation average: %.2f%%\" % (average_accuracy * 100))\n",
    "        logging.info('-'*80)\n",
    "\n",
    "        # Evolve, except on the last iteration.\n",
    "        if i != generations - 1:\n",
    "            # Do the evolution.\n",
    "            networks = optimizer.evolve(networks)\n",
    "\n",
    "    # Sort our final population.\n",
    "    networks = sorted(networks, key=lambda x: x.accuracy, reverse=True)\n",
    "\n",
    "    # Print out the top 5 networks.\n",
    "    print_networks(networks[:5])\n",
    "\n",
    "def print_networks(networks):\n",
    "    \"\"\"Print a list of networks.\n",
    "\n",
    "    Args:\n",
    "        networks (list): The population of networks\n",
    "\n",
    "    \"\"\"\n",
    "    logging.info('-'*80)\n",
    "    for network in networks:\n",
    "        network.print_network()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Evolve a network.\"\"\"\n",
    "    generations = 2  #10 Number of times to evole the population.\n",
    "    population = 5  #20 Number of networks in each generation.\n",
    "    dataset = 'mnist'\n",
    "\n",
    "    nn_param_choices = {\n",
    "        'nb_neurons': [64, 128, 256, 512, 768, 1024],\n",
    "        'nb_layers': [1, 2, 3, 4],\n",
    "        'activation': ['relu', 'elu', 'tanh', 'sigmoid'],\n",
    "        'optimizer': ['rmsprop', 'adam', 'sgd', 'adagrad',\n",
    "                      'adadelta', 'adamax', 'nadam'],\n",
    "    }\n",
    "\n",
    "    logging.info(\"***Evolving %d generations with population %d***\" %\n",
    "                 (generations, population))\n",
    "\n",
    "    generate(generations, population, nn_param_choices, dataset)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
